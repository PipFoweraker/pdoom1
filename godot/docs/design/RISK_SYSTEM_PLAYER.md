# Consequences: How Your Decisions Compound

> *"The seeds of catastrophe are often planted years before the harvest."*

## The Hidden Cost of Shortcuts

In PDoom, not every consequence is immediate. When you rush a paper to meet a deadline, the doom meter might not spike right away. But somewhere in the background, you've increased the probability that something will go wrong later.

This is how the real world works. A lab that consistently cuts corners doesn't explode on day one. Instead, small compromises accumulate until, one day, a paper gets retracted, a funder pulls out, or a researcher burns out and leaks sensitive data.

Your decisions create **risk** - and risk eventually comes due.

---

## What Creates Risk?

Every choice you make shifts the balance. Some examples:

**Rushing to publish** might get your paper out faster, but it increases the chance of:
- Future retractions damaging your reputation
- Your work being cited in ways you didn't intend
- Contributing to capabilities racing ahead of safety

**Hiring aggressively** before securing funding might let you scale quickly, but it increases the chance of:
- Painful layoffs if a grant falls through
- Cash flow crises that force desperate decisions
- Salary disparities that breed resentment

**Staying quiet** while competitors make headlines might feel safer, but it increases the chance of:
- Being blindsided by regulations written without your input
- Missing opportunities that go to more visible labs
- Losing top talent to labs with better reputations

There's no "safe" path. Every strategy has trade-offs. The question is: which risks are you willing to accept?

---

## How Risk Manifests

You won't see a "Risk Meter" climbing. Instead, you'll experience the consequences through **events**.

Some events are clearly connected to your choices:
> *"A paper your lab rushed last year has been flagged for reproducibility issues. Your reputation takes a hit."*

Others feel more like bad luck - but they weren't random:
> *"A key researcher has accepted an offer from a competitor. The departure comes as a surprise, but morale has been low for months."*

How much you understand about *why* things happen depends on your character's skills.

---

## Insight: Seeing the Threads

Not all lab directors are equally perceptive. Your **Insight** skills determine how much the game reveals about causes and consequences.

**Low Insight** - You see outcomes, but connections are murky:
> *"Reaction to this will likely be mixed."*
> *"Something feels off about morale lately."*

**High Insight** - You see the machinery:
> *"Open Philanthropy will seriously dislike this approach. Anthropic will be neutral. This may affect your next funding round."*
> *"Your rushed publication last quarter is being cited in a capabilities paper. Regulatory attention is increasing."*

Insight can be developed through:
- Character creation (allocate points at game start)
- Upgrades and investments during play
- Experience in specific domains (financial insight vs. political insight)

Some players prefer the challenge of operating with limited information. Others want to see the full decision tree. Build your character accordingly.

---

## The Six Dimensions of Risk

Your lab faces pressure from multiple directions:

### Capability Overhang
The gap between what AI systems can do and what we understand about making them safe. When capabilities advance faster than safety, the world becomes more dangerous - regardless of your intentions.

### Research Integrity
The trustworthiness of your lab's work. Cutting corners, skipping peer review, or ignoring failed replications might save time, but erodes the foundation your reputation is built on.

### Regulatory Attention
Governments pay attention to AI labs - especially ones that make headlines. High visibility can mean opportunity, but it also means scrutiny. Sometimes the best move is to stay under the radar.

### Public Awareness
The general public is increasingly aware of AI risks. Their attention can be a resource (funding, support) or a liability (protests, political pressure). Managing public perception is part of the job.

### Insider Threat
Your biggest risks might come from within. Overworked researchers make mistakes. Underpaid staff take other offers - or worse. A healthy organization is a secure organization.

### Financial Exposure
Grants take months to arrive. Overhead costs are easy to underestimate. Hiring ahead of confirmed funding feels bold until the runway runs out. Financial fragility constrains every other decision you make.

---

## Reducing Risk

Risk doesn't decay on its own. If you've been cutting corners, simply stopping won't fix the problem - you've already planted the seeds.

To actually reduce risk, you need to take **active countermeasures**:

- **Publish safety research** to close the capability-safety gap
- **Publish negative results** to rebuild research credibility
- **Invest in compliance** to ease regulatory concerns
- **Engage the public** constructively to shape the narrative
- **Improve working conditions** to retain and motivate staff
- **Maintain cash reserves** to weather funding delays

These actions have costs. They take time, money, and attention away from other priorities. But they're how you pay down the debt from past compromises.

---

## The Long Game

PDoom is a game about managing complex systems over time. The decisions that matter most aren't the dramatic ones - they're the habits you form, the corners you cut (or don't), the culture you build.

A lab that consistently prioritizes rigor over speed will face different problems than one that moves fast and breaks things. Neither path is inherently right or wrong. But you should make these choices deliberately, understanding what you're trading for what.

The doom meter shows where you are. The risks you've accumulated determine where you're going.

Choose wisely.

---

## Tips for New Players

1. **Early decisions compound.** Habits formed in the first few years set the tone for everything after.

2. **Watch for warning signs.** Events often hint at underlying problems before they become crises.

3. **Diversify your risks.** A lab that's vulnerable on all fronts won't survive long. Pick your battles.

4. **Build buffers.** Cash reserves, strong culture, and solid reputation give you room to absorb shocks.

5. **Learn from failure.** When bad events happen, think about what decisions led there. The game rewards players who adapt.

---

*This system is inspired by real dynamics in research institutions, AI labs, and the effective altruism community. The tensions between speed and safety, visibility and scrutiny, ambition and sustainability are not hypothetical - they're playing out right now.*
